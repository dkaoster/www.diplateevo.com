<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta name='viewport' content='width=device-width,initial-scale=1.0'>

    <link rel="preload" as="style" href="/reset.css" />
    <link rel='stylesheet' href='/reset.css' media="print" onload="this.media='all'" />
    <noscript>
      <link rel='stylesheet' href='/reset.css'>
    </noscript>

    <!-- The Fastest Google Fonts https://csswizardry.com/2020/05/the-fastest-google-fonts/ -->
    <!-- [1] -->
    <link rel="preconnect"
          href="https://fonts.gstatic.com"
          crossorigin />

    <!-- [2] -->
    <link rel="preload"
          as="style"
          href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&family=Noto+Sans+TC&display=swap" />

    <!-- [3] -->
    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&family=Noto+Sans+TC&display=swap"
          media="print" onload="this.media='all'" />

    <!-- [4] -->
    <noscript>
      <link rel="stylesheet"
            href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&family=Noto+Sans+TC&display=swap" />
    </noscript>

    <!-- This contains the contents of the <svelte:head> component -->
    
		<link href="../_app/immutable/assets/0.Cu4ryLhb.css" rel="stylesheet">
		<link href="../_app/immutable/assets/4.CzgFuw8o.css" rel="stylesheet">
		<link href="../_app/immutable/assets/PortfolioItem.BZTESynL.css" rel="stylesheet">
		<link href="../_app/immutable/assets/ArchieMLMarkdownHTML.Btt4otIb.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.ChEkEh4p.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/entry.BcNDoklU.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/runtime.D8xlN6v3.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/index.CPyn8phU.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/control.CYgJF_JY.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.7Jp1DJ0Y.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/preload-helper.Q1qzfZOP.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/disclose-version.DxIN9GmO.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/if.C19_1zQV.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/this.DzTm2LiH.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.DzcMb2lc.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/legacy.BogHneBK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/attributes.CWqA2rVy.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/class.DRlD_2ny.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/i18n.BFt5D78s.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/4.CNTGdw0Z.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/4.BnRq_Kyo.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/index.g5YcAAdQ.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/image.D5wzb525.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Helmet.B5U6eGSi.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/await.DcdeniSk.js"><!--[--><link rel="icon" href="/favicon.svg" type="image/svg+xml"> <link rel="icon" href="/favicon.png" type="image/png"> <link rel="icon" href="/favicon.ico" type="image/x-icon"> <meta name="title" content="Intro to HCI Lecture 6 – Testing"> <!--[!--><!--]--> <!--[--><meta name="description" content="These are my notes in essay form from the Intro to Human Computer Interaction course taught by Scott Klemmer at the University of California, San Diego."><!--]--> <meta property="og:title" content="Intro to HCI Lecture 6 – Testing"> <meta property="og:type" content="article"> <meta property="og:url" content="https://www.diplateevo.com/intro-to-hci-lecture-6-testing/"> <meta property="og:image" content="https://www.diplateevo.com/images/2020/10/2015-01-31-03.32.46-1.jpg"> <meta property="og:site_name" content="Diplateevo"> <meta property="og:locale" content="en_US"> <!--[--><meta property="og:description" content="These are my notes in essay form from the Intro to Human Computer Interaction course taught by Scott Klemmer at the University of California, San Diego."><!--]--> <meta name="twitter:image" content="https://www.diplateevo.com/images/2020/10/2015-01-31-03.32.46-1.jpg"> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:title" content="Intro to HCI Lecture 6 – Testing"> <meta name="twitter:description" content="These are my notes in essay form from the Intro to Human Computer Interaction course taught by Scott Klemmer at the University of California, San Diego."> <!--[--><script defer="" data-domain="diplateevo.com" src="https://plausible.io/js/plausible.js"></script><!----><!--]--> <!----><!--[!--><!--]--><!----><!--]--><title>Intro to HCI Lecture 6 – Testing - Diplateevo</title>

  </head>
  <body>
    <!-- The application will be rendered inside this element -->
    <!--[--><!--[--><!----><div class="svelte-bmidcl "><nav class="wrapper svelte-8zm883"><div class="fill svelte-8zm883"></div> <div class="header svelte-8zm883"><a class="logo-link svelte-8zm883" href="/" rel="prefetch"><div class="logo svelte-8zm883">高</div></a> <div class="nav svelte-8zm883"><a class="nav-link svelte-8zm883" href="/" rel="prefetch">Home</a> <!--[--><a class="nav-link svelte-8zm883" href="/about" rel="prefetch">about</a><a class="nav-link svelte-8zm883" href="/works" rel="prefetch">works</a><!--]--> <button class="theme-toggle svelte-8zm883"><!--[!--><svg width="24" viewBox="0 0 47.167 47.167" style="enable-background:new 0 0 47.167 47.167; display: block"><path style="fill: var(--color);" d="M46.369,28.793c-11.852,5.935-26.271,1.138-32.206-10.714c-2.748-5.488-3.191-11.524-1.702-17.016
	C1.197,7.236-3.255,21.263,2.544,32.844C8.479,44.696,22.898,49.493,34.75,43.558c6.364-3.187,10.69-8.821,12.417-15.19
	C46.903,28.513,46.64,28.658,46.369,28.793z"></path></svg><!--]--></button></div></div></nav><!----> <!----><!----><!----> <article class="grid-wrap svelte-12738yp " lang="en"><div class="grid svelte-12738yp"><h1 class="title svelte-12738yp ">Intro to HCI Lecture 6 – Testing</h1> <!--[--><!--[--><p class="summary svelte-12738yp ">These are my notes in essay form from the Intro to Human Computer Interaction course taught by Scott Klemmer at the University of California, San Diego.</p><!--]--> <div class="cover-image svelte-12738yp"><div class="image-wrap cover svelte-1p9e9k9"><!--[--><!--[--><picture class="svelte-1p9e9k9"><!--[--><source media="(max-width: 640px)" srcset="/images/2020/10/2015-01-31-03.32.46-1@640w.jpg" type="image/jpeg" class="svelte-1p9e9k9"><source media="(max-width: 768px)" srcset="/images/2020/10/2015-01-31-03.32.46-1@768w.jpg" type="image/jpeg" class="svelte-1p9e9k9"><source media="(max-width: 1280px)" srcset="/images/2020/10/2015-01-31-03.32.46-1@1280w.jpg" type="image/jpeg" class="svelte-1p9e9k9"><source media="(max-width: 1920px)" srcset="/images/2020/10/2015-01-31-03.32.46-1@1920w.jpg" type="image/jpeg" class="svelte-1p9e9k9"><!--]--> <img loading="eager" alt="photograph" src="/images/2020/10/2015-01-31-03.32.46-1.jpg" width="100%" class="svelte-1p9e9k9"></picture><!--]--><!--]--> <!--[!--><!--]--></div><!----></div><!--]--> <!--[--><div class="meta svelte-12738yp">Apr 2015</div><!--]--> <!--[!--><!--]--> <!----><!--[--><!--[--><!----><p>These are my notes in essay form from the Intro to Human Computer Interaction course taught by <a href="https://d.ucsd.edu/srk/" target="_blank" rel="noopener noreferrer">Scott Klemmer</a> at the University of California, San Diego. All credit for content goes to Scott, any errors are my own.</p>
<!----><!--]--><!--[--><!----><p>The next step in the process is to conduct tests on the actual usage of the application. Just like what we saw in the needfinding and interviewing process, simply asking people what they want will lead you to miss a lot of things. Questions like “do you like the interface?” tell you very little about how to improve your product. Even questions that appear to drive more specific results, such as rating the interface on a scale of one to ten, may appear to be more scientific, but is essentially the same question.</p>
<!----><!--]--><!--[--><!----><p>One reason that these questions are relatively unhelpful is because they are not very specific. There is often nothing to compare an interface to when asking that question, leaving the experimenter with a subjective answer.</p>
<!----><!--]--><!--[--><!----><p>There is also what is known as the “please the experimenter bias”, which is the effect of the tester doing their best to please the experimenter, which can be exacerbated when the experimenter is from a different social background, such as when white researchers conducted interviews in foreign countries like India.</p>
<!----><!--]--><!--[--><!----><h2 id="characteristics-of-good-research-questions">Characteristics of Good Research Questions</h2>
<!----><!--]--><!--[--><!----><p>As with user interviews and getting feedback, getting beyond “do you like my interface” means asking questions to that are specific and provide specific insights. Here are a couple characteristics of good questions.</p>
<!----><!--]--><!--[--><!----><ul>
<li><strong>Baserates</strong> – Measuring a baserate generally involves measuring how often something happens, such as how many people click the first link on a search results page.</li>
<li><strong>Correlations</strong> – Measuring correlations means discovery connections between two separate events, such as measuring the correlation between when a class is offered and how many students attend.</li>
<li><strong>Causes</strong> – However, with any correlation, be aware that there can be multiple explanations. For whether or not students attend class, the correlation can be a function of when students want to go to class or when good professors want to teach.</li>
</ul>
<!----><!--]--><!--[--><!----><h2 id="research-terms-to-know">Research Terms to Know</h2>
<!----><!--]--><!--[--><!----><p>To fully understand the way that research is conducted, there are a couple of research terms to be aware of.</p>
<!----><!--]--><!--[--><!----><ul>
<li><strong>Independent variables</strong> are variables that are independent of what the user does, because they are manipulated by the experimenter or application. In the example of the experiment of classes, the independent variable is the time the class is, because that is what the experimenter has control over.</li>
<li><strong>Dependent variables</strong>, on the other hand, are variables that the user has control of and that the experimenter is trying to measure. Common dependent variables include how long it takes for a user to complete a task, how many errors people make, how much a person remembers after an action, or emotional response.</li>
<li><strong>Internal validity</strong> determines how reliable an experiment is. If you were to run the experiment again, would you get similar results? Or, if you were to run the experiment with a greater number of people, would the results be similar?</li>
<li><strong>External validity</strong> is the generalizability of the results. Because internal validity may not apply in a real world situation, external validity determines how easily the results can be applied to other situations.</li>
</ul>
<!----><!--]--><!--[--><!----><p>One of the common questions that designers want to ask is “is my cool new approach better than the industry standard?” After all, that is the reason why we are building something new. However, the problem with this question is that it is not a fair comparison. The industry standard is likely to have years of development and improvement as well as users that have adapted to the standard, while your product is likely a brand new one without too much development and familiarity. Consequently, asking if your approach is better than the industry standard can generate a response that is attributed to fidelity, approach, or a combination of both.</p>
<!----><!--]--><!--[--><!----><p>For example, one research study that came out when the iPhone was launched seeked to see if the touchscreen keyboard on the iPhone is better than physical qwerty keyboards or numeric keyboards.</p>
<!----><!--]--><!--[--><!----><blockquote>
<p>“Research firm User Centric has released a study that tries to gauge how effective the iPhone’s unusual on-screen keyboard is. The goal is certainly a noble one, but I can’t say that the survey’s approach results in data that makes much sense.</p>
</blockquote>
<!----><!--]--><!--[--><!----><blockquote>
<p>User Centric brought in twenty owners of other phones — half who had ones with QWERTY keyboards, and half who had ordinary numeric phone keypads. None were familiar with the iPhone. The research involved having the test subjects enter six sample text messages with the phones they already had, and six with an iPhone.</p>
</blockquote>
<!----><!--]--><!--[--><!----><blockquote>
<p>Logical end result: These iPhone newbies took twice as long to enter text with an iPhone as they did with their own phones, and made lots more typos.”</p>
</blockquote>
<!----><!--]--><!--[--><!----><p>Let’s break down this study to see why it was not a very good study. In this study, the independent variable, or the manipulation, was the input style. The measure, or metric for comparison, was words per minute.</p>
<!----><!--]--><!--[--><!----><p>In this study, all of the participants were new to iPhone keyboards, making them novices at the iPhone’s new style of keyboard, while being experts at the other type of keyboard they were already used to. While there is nothing wrong with testing beginners to see how fast people can learn a new keyboard, comparing their performance to a keyboard that they are used to is an intrinsically unfair comparison.</p>
<!----><!--]--><!--[--><!----><p>When the study was performed again on participants a month later, they found that iPhone users were about as fast as QWERTY users. This comparison is a more equivalent comparison than the first one.</p>
<!----><!--]--><!--[--><!----><p>However, when looking at the error rate of each type of input device, we find that iPhone users make more errors than the QWERTY or numeric keyboard users. Based on the nature of this study, this result can potentially be traced back to the fact that each keyboard type was used by a different group of users. Thus, there could have been significant differences in who each group of people were, making the variance in the user population still a problematic function if we want to draw conclusions about the overall performance of an input style.</p>
<!----><!--]--><!--[--><!----><h2 id="strategies-for-fair-comparisons">Strategies for Fair Comparisons</h2>
<!----><!--]--><!--[--><!----><p>Now that you’ve seen different ways that comparisons can be unfair, it is easier to recognize that an unfair comparison can lead to skewed results and inaccurate conclusions. Therefore, your job as a researcher is to make comparisons as fair as possible.</p>
<!----><!--]--><!--[--><!----><p>The first strategy for a fairer comparison is to insert your new approach into the actual production setting. By swapping out only the part of the approach that you are trying to change, you can easily see the difference that your approach makes. This may seem challenging, but you can use proxy servers or client side scripting to easily make this work.</p>
<!----><!--]--><!--[--><!----><p>The second strategy for a fairer comparison is to recreate the production approach in your new setting. Similar to the first strategy, this is another way to compare only the parts that you want to change into the production approach.</p>
<!----><!--]--><!--[--><!----><p>Another strategy that is used commonly in research is to scale things down so that you’re just looking at a piece of a larger system. Instead of building the entirety of the program, scaling things down to only the action that your change affects allows you to easily compare the differences.</p>
<!----><!--]--><!--[--><!----><p>Finally, when expertise is relevant, train people up. As we see in the study with the iPhone keyboards, people who were using the iPhone keyboards for the first time unsurprisingly performed worse than people who were using keyboards that they were familiar with.</p>
<!----><!--]--><!--[--><!----><p>The answer to whether interface x is better than interface y is most often “it depends”. Without defining what “better” means, it is impossible to quantitatively compare two interfaces. On the contrary, by properly understanding these characteristics of experiments, it allows you to make more accurate conclusions on the data that you collect.</p>
<!----><!--]--><!--[!--><!--[--><div class="image-wrap  svelte-1p9e9k9"><!--[--><!--[--><picture class="svelte-1p9e9k9"><!--[--><!--]--> <img loading="lazy" alt="Correlation Comic" src="/images/2021/01/correlation.png" width="400" class="svelte-1p9e9k9"></picture><!--]--><!--]--> <!--[!--><!--]--></div><!--]--><!--]--><!--[--><!----><h2 id="in-person-experiments">In Person Experiments</h2>
<!----><!--]--><!--[--><!----><p>While the web has made running experiments online extremely easy, there are also many benefits that come with watching people in person. By watching people live, you can talk to them about what they’re doing, hear their feedback, see points of confusion, and in general have a higher bandwidth of engagement.</p>
<!----><!--]--><!--[--><!----><p>When planning an in person experiment, the first step is to make clear goals and lay out the scope of the experiment, which will be guided by the purpose of the experiment, or what you hope to learn.</p>
<!----><!--]--><!--[--><!----><p>For example, if you’re building a room reservation system for a computer science department, one strategy would be to put all the information on one page, while another strategy may be putting information on multiple pages. You might be interested in seeing if splitting things up increases the likelihood of booking a room or not.</p>
<!----><!--]--><!--[--><!----><p>To conduct this experiment, we must consider a couple of factors. First, identify an appropriate setting for testing. If your software is most likely to be used in a quiet room, find a quiet room to do your testing. If your software is most likely to be used at a train station, conduct your experiment at a train station.</p>
<!----><!--]--><!--[--><!----><p>Next, figure out who you’re going to recruit, how, and how many people. Come up with realistic scenarios, ideally ones that participants can care about, to give people when they come in.</p>
<!----><!--]--><!--[--><!----><p>When it comes time for people to come in to test your product, have a list of questions and things that you want to pay attention to. In the case of our example, questions may include “Will people book the right sized room? For the right length of time?” or “How long did it take for the participant to book the room?” or “What other actions do people do while booking a room?”</p>
<!----><!--]--><!--[--><!----><p>If you can, it is helpful to have at least two experimenters present for the test. One person will be in charge of facilitating while the other takes notes.</p>
<!----><!--]--><!--[--><!----><p>During in person experiments, it is important to create concrete tasks for people to do, so that you have concrete data to compare, and everyone gets the same experience. In our example, a concrete task prompt might look something like:</p>
<!----><!--]--><!--[--><!----><blockquote>
<p>“Book a room sometime next week for a research group meeting. Andrew will be out of town, so we won’t hear his weekly update. The rest of us should be present and give our updates. Besides the usual group members, we’ll have two visitors from France who will present their research — maybe they’ll take ten minutes each. When you’re done booking the room, tell Arvind so he can prepare the next task for you.”</p>
</blockquote>
<!----><!--]--><!--[--><!----><p>As a side note, something to consider when performing in person experiments are ethical considerations. Some experiments have left participants in tears when they were unable to perform well. For starters, make consent voluntary, and make sure consent is informed, and allow participants to stop the study whenever they want. Perhaps the biggest clarification is to inform the user that you are testing the site, not the users. This helps users to understand that mistakes are the fault of the system instead of their own.</p>
<!----><!--]--><!--[--><!----><p>Another tip to create better experiments is to pilot your experimenting, letting one or two people go through the experiment before bringing in larger amounts of participants. This will help you work out any kinks in the flow of the experiment, the order of the experiment, and any other cases that may arise.</p>
<!----><!--]--><!--[--><!----><p>To best capture results from your experiment, the most simple and direct way is to take notes of things that went well, questions that arose, or problems to fix, one option is to record video. Be sure to collect both qualitative process data such as observations of what users are thinking and doing, as well as bottom-line quantitative data such as how many people succeed, how long it takes, etc.</p>
<!----><!--]--><!--[--><!----><p>Going beyond simple note taking, you may choose to record video. Recording video allows you to document and easily share exactly how and what the user felt as they go through the process.</p>
<!----><!--]--><!--[--><!----><p>Another consideration to capturing results is to determine whether you want participants to think aloud while they are going through the process. This allows you to record what the users are thinking, not just what they are doing. Since most people may not be used to sharing their thoughts out loud, you may need to prompt users along the way. Be aware that thinking out loud may also cause users behavior’s to change, and may not be an exact representation of what the user is thinking.</p>
<!----><!--]--><!--[--><!----><p>At the end of the experiment, it is helpful to debrief the participant so that they know what your goals were and so that you can learn more holistically what they are thinking after the experiment.</p>
<!----><!--]--><!--[--><!----><h2 id="online-experiments">Online Experiments</h2>
<!----><!--]--><!--[--><!----><p>The web has enabled tremendous power to conduct experiments online. This is accomplished by rolling out different versions of the user interface, getting feedback, and iterating quickly. You may have heard of this under many different names, likely A/B testing, split testing, randomized experiments, etc. The basic premise for this kind of testing is to randomly split traffic on your website to two or more versions of your interface, and then collect data on how the two versions perform.</p>
<!----><!--]--><!--[--><!----><p>As an example, the following are pages for the national alert registry, with three versions laid out below. The third interface, a two column layout, was designed to see whether it would generate more sign ups because the content is available above the fold.</p>
<!----><!--]--><!--[!--><!--[--><div class="image-wrap full svelte-1p9e9k9"><!--[--><!--[--><picture class="svelte-1p9e9k9"><!--[--><source media="(max-width: 640px)" srcset="/images/2021/01/Screenshot-2015-04-03-20.14.10@640w.png" type="image/png" class="svelte-1p9e9k9"><source media="(max-width: 768px)" srcset="/images/2021/01/Screenshot-2015-04-03-20.14.10@768w.png" type="image/png" class="svelte-1p9e9k9"><source media="(max-width: 1280px)" srcset="/images/2021/01/Screenshot-2015-04-03-20.14.10@1280w.png" type="image/png" class="svelte-1p9e9k9"><source media="(max-width: 1920px)" srcset="/images/2021/01/Screenshot-2015-04-03-20.14.10@1920w.png" type="image/png" class="svelte-1p9e9k9"><!--]--> <img loading="lazy" alt="Screenshot of national alert registry" src="/images/2021/01/Screenshot-2015-04-03-20.14.10.png" width="100%" class="svelte-1p9e9k9"></picture><!--]--><!--]--> <!--[!--><!--]--></div><!--]--><!--]--><!--[--><!----><p>Before scrolling down to see the results, think about which version of these three user interfaces you think was the most effective.</p>
<!----><!--]--><!--[!--><!--[--><div class="image-wrap  svelte-1p9e9k9"><!--[--><!--[--><picture class="svelte-1p9e9k9"><!--[--><source media="(max-width: 640px)" srcset="/images/2021/01/Screenshot-2015-04-03-20.13.37@640w.png" type="image/png" class="svelte-1p9e9k9"><source media="(max-width: 768px)" srcset="/images/2021/01/Screenshot-2015-04-03-20.13.37@768w.png" type="image/png" class="svelte-1p9e9k9"><!--]--> <img loading="lazy" alt="Screenshot of table" src="/images/2021/01/Screenshot-2015-04-03-20.13.37.png" width="100%" class="svelte-1p9e9k9"></picture><!--]--><!--]--> <!--[!--><!--]--></div><!--]--><!--]--><!--[--><!----><p>What you can see from the data is that the third version actually performed worse than the first two. Although it was intended to be more effective, the number of sales actually dropped tremendously. Even the very best designers often make a revision that performs worse than the original. The web allows us to easily try out these designs and get real feedback on how the designs perform.</p>
<!----><!--]--><!--[--><!----><p>So why did the third version do so much worse? One theory is the word free at the top left of the screen, that can dissuade people from buying something. Another theory is that by creating two columns, there is less of a clear flow, creating a perception of multiple options instead of a clear funnel that you want people to go through. By contrast, the second version did not have the word free at the top, and had a stronger call to action.</p>
<!----><!--]--><!--[--><!----><p>Therefore, we can see from this example that small changes can have a big effect on the performance of a site. These changes can include things like the position and color of the primary call to action, position of testimonials on the page, whether linked elements are in text or images, the amount of white space on a page, the position and prominence of the main heading, the number of columns on the page, number of visual elements competing for attention, the age, sex, and appearance of someone in a photo, etc. See the <a href="/intro-to-hci-lecture-5-visual-design">visual design</a> lecture for more details on how to create good visual design.</p>
<!----><!--]--><!--[--><!----><p>Another example is Obama’s presidential campaign in 2008. Dan Siroker of the Obama campaign, shares how he used simple A/B tests on the button as well as the media to increase the performance of the Obama’s campaign site.</p>
<!----><!--]--><!--[!--><!--[!--><!--[!--><!--[--><!----><iframe width="100%" height="400" src="https://www.youtube.com/embed/71bH8z6iqSc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><!----><!--]--><!--]--><!--]--><!--]--><!--[--><!----><p>In the experiment with the text on the buttons, the only thing that changed between versions is the text on the button. The four choices were “Sign Up”, “Learn More”, “Join Us Now”, and “Sign Up Now”. Turns out, “Learn More” performed 18% better than the baseline.</p>
<!----><!--]--><!--[--><!----><p>The second experiment, the media on the page, was split between three photos and three videos. The strongest media, contrary to what most people would have guessed, was the family photo.</p>
<!----><!--]--><!--[--><!----><p>From these examples, there are a couple of general principles that contribute to a quality online experiments.</p>
<!----><!--]--><!--[--><!----><p>Firstly, when running online experiments, it is most helpful to run experiments at 50/50%. While some people recommend starting a small percentage of users with the new design, this is not as helpful for being able to quickly tell which interface performs better. By setting it at 50/50, the better interface is blatantly apparent. However, to prevent disastrous results, ramping up to 50% is the best course of action. Ramping up will help avoid software bugs or other unforeseen effects affecting users from performing the task.</p>
<!----><!--]--><!--[--><!----><p>There are some key rules for random assignment. The first is that your assignment should be consistent. The same person should see the same interface every single time they use your interface. The second key rule is that your assignment should be truly and completely random. This means that you want to run different versions simultaneously, and that the probability someone has of using one version of the interface is the same as using the another version.</p>
<!----><!--]--><!--[--><!----><p>The hardest part about online experimentation is picking the metric to measure a good interface by. One common mistake when picking metrics is picking a metric that is easy to measure instead of a metric that accurately reflects success. Thus, it is important to pick a metric that matters over picking a metric that is easy to measure.</p>
<!----><!--]--><!--[--><!----><p>Additionally, it is also important to run online experiments for long enough. Just like users testing the new iPhone keyboard, if you change a user interface, people may not be used to it, generating poorer initial results.</p>
<!----><!--]--><!--[--><!----><p>Finally, no matter how long or diligently you conduct your experiments, larger theories are still difficult to prove. Even with large sets of data, the variance and changes can be due to a larger, unseen reason. Thus, it is important to constantly ask different questions and run multiple types of experiments to identify trends on how interfaces perform rather than all encompassing claims one way or the other.</p>
<!----><!--]--><!--[--><!----><h2 id="interpreting-test-results">Interpreting Test Results</h2>
<!----><!--]--><!--[--><!----><p>After you’ve done your tests, whether in person or online, it is important to properly analyze the data so that you reach accurate conclusions. A good starting point is asking yourself the questions “What does my data look like?”, “What are the overall numbers?” and “Are the differences real?”</p>
<!----><!--]--><!--[--><!----><p>For instance, say I have a coin and I want to test to see whether the coin is a fair coin or not. To test the coin for fairness, let’s say that I flip it 20 times and 13 of the flips came up heads. Despite the fact that the result of 13 heads is not equal to the expected number of heads, we need to determine if 3 additional heads out of 20 is a significant difference.</p>
<!----><!--]--><!--[--><!----><p>Since the results of flipping a coin lies on a gaussian distribution, that is that the further away the outcome is from the mean, the less likely it is to occur, we know that flipping 11 heads on a fair coin out of 20 flips is more likely than flipping 19 heads out of 20 flips.</p>
<!----><!--]--><!--[!--><!--[--><div class="image-wrap  svelte-1p9e9k9"><!--[--><!--[--><picture class="svelte-1p9e9k9"><!--[--><!--]--> <img loading="lazy" alt="normal distribution" src="/images/2021/01/fig11-10.jpg" width="300" class="svelte-1p9e9k9"></picture><!--]--><!--]--> <!--[!--><!--]--></div><!--]--><!--]--><!--[--><!----><p>To determine the significance of the difference, we are going to use the <a href="https://en.wikipedia.org/wiki/Pearson's_chi-squared_test" target="_blank" rel="noopener noreferrer">Pearson’s Chi-Squared Test</a>. This test is a way to compare the expected rates to the experimental rates to determine a percentage of confidence toward a conclusion.</p>
<!----><!--]--><!--[!--><!--[--><div class="image-wrap  svelte-1p9e9k9"><!--[--><!--[--><picture class="svelte-1p9e9k9"><!--[--><!--]--> <img loading="lazy" alt="confidence equation x^{2} = \frac{(observed - expected)^{2}}{expected}" src="/images/2021/01/latex.png" width="300" class="svelte-1p9e9k9"></picture><!--]--><!--]--> <!--[!--><!--]--></div><!--]--><!--]--><!--[--><!----><p>To accurately determine whether we can reject the null hypothesis, or the default hypothesis that the coin is a fair coin, we need a threshold p-value, most often 0.05, to compare with the result of the chi-squared test.</p>
<!----><!--]--><!--[--><!----><p>Therefore, if 20 flips of a coin resulted in 13 heads and we have a p-value less than 0.05, we have:</p>
<!----><!--]--><!--[!--><!--[--><div class="image-wrap  svelte-1p9e9k9"><!--[--><!--[--><picture class="svelte-1p9e9k9"><!--[--><!--]--> <img loading="lazy" alt="\frac{(13 - 10)^{2}}{10} + \frac{(7 - 10)^{2}}{10} = 1.8" src="/images/2021/01/latex--1-.png" width="300" class="svelte-1p9e9k9"></picture><!--]--><!--]--> <!--[!--><!--]--></div><!--]--><!--]--><!--[--><!----><p>(heads + tails = chi-squared value)</p>
<!----><!--]--><!--[--><!----><p>Based on our calculation and the <a href="http://www.socscistatistics.com/pvalues/chidistribution.aspx" target="_blank" rel="noopener noreferrer">p-value distribution table</a>, we can see that our results are not quite at the p &lt; 0.05 threshold, meaning that we cannot sufficiently reject our null hypothesis and say that the coin is biased.</p>
<!----><!--]--><!--[--><!----><p>However, if we flipped the coin 60 times and resulted in 39 heads, the same ratio as the previous experiment, running those numbers through the chi-squared distribution allows us to reject the null hypothesis with 98% certainty. This is because more trials brings our chi-squared value up.</p>
<!----><!--]--><!--[--><!----><p>Chi-Squared testing is valuable because it doesn’t just apply to coins, but also applies to click through rates on websites. Let’s say that a website has a button labeled “sign up”, and 10% of visitors click the button. To try to improve traffic, designers change the button to “learn more”, and start gathering data. Over a week, there were 1000 visitors to the site, 118 of which clicked the “learn more” button. Running this data through the formula gives us the follow chi-squared value:</p>
<!----><!--]--><!--[!--><!--[--><div class="image-wrap  svelte-1p9e9k9"><!--[--><!--[--><picture class="svelte-1p9e9k9"><!--[--><!--]--> <img loading="lazy" alt="\frac{(118 - 100)^{2}}{100} + \frac{(882 - 900)^{2}}{900} = 4.01" src="/images/2021/01/latex--2-.png" width="300" class="svelte-1p9e9k9"></picture><!--]--><!--]--> <!--[!--><!--]--></div><!--]--><!--]--><!--[--><!----><p>In looking up this data in <a href="http://www.socscistatistics.com/pvalues/chidistribution.aspx" target="_blank" rel="noopener noreferrer">our table</a>, we find that 4.01 generates a p-value that is just smaller than 0.05, allowing us to formally say that the change was an improvement.</p>
<!----><!--]--><!--[--><!----><p>Thus, statistical testing gives you two very powerful tools. First, it allows you to formalize “we’re pretty sure”, using statistics to validate your claim. Secondly, it helps you generalize (or not) from small sample sizes. There are also other tests that achieve a similar purpose for continuous data such as <a href="https://en.wikipedia.org/wiki/Student's_t-test" target="_blank" rel="noopener noreferrer">T-tests</a> and <a href="https://en.wikipedia.org/wiki/Analysis_of_variance" target="_blank" rel="noopener noreferrer">ANOVA</a>.</p>
<!----><!--]--><!--[--><!----><p>Also, note that data often isn’t “normal”. Data doesn’t always lie on a gaussian distribution, as it can be bimodal or weighted one way, making statistical analysis potential complex.</p>
<!----><!--]--><!--[--><!----><h2 id="further-reading-and-resources">Further Reading and Resources</h2>
<!----><!--]--><!--[--><!----><ul>
<li><a href="https://faculty.washington.edu/wobbrock/pubs/hcic-11.pdf" target="_blank" rel="noopener noreferrer">Practical Statistics for HCI – Jacob Wobbrock</a></li>
<li><a href="https://www.amazon.com/gp/product/0495115770?linkId=I66BIBQDKUFJCBKA" target="_blank" rel="noopener noreferrer">Doing Psychology Experiments – David W Martin</a></li>
<li><a href="https://www.amazon.com/gp/product/0805805281?linkId=YEHNHW7DK43YYWRL" target="_blank" rel="noopener noreferrer">Statistics as Principled Argument – Robert P. Abelson</a></li>
<li><a href="https://www.amazon.com/gp/product/0335216803?linkId=YGYS6V3HD5CQXE7L" target="_blank" rel="noopener noreferrer">Learning to use Statistical Tests in Psychology – Judith Greene, Manuela D’Oliveira</a></li>
</ul>
<!----><!--]--><!--]--><!----><!----></div></article><!----><!----></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			<script type="application/json" data-sveltekit-fetched data-url="/content/pages.json">{"status":200,"statusText":"","headers":{},"body":"[{\"slug\":\"about\",\"isPage\":true,\"hideRSS\":false,\"en\":{\"title\":\"Graphics Journalist, Digital Cartographer, Engineer, Noodle Connoisseur\",\"isPage\":\"true\",\"description\":\"I spend my days crunching data and visualizing news stories for the web and mobile devices.\",\"slug\":\"about\",\"locale\":\"en\"},\"zh\":{\"title\":\"資料視覺化前端工程師、數位地理資訊學者、湯麵愛好者\",\"isPage\":\"true\",\"description\":\"我專門發想以及製作新聞和資料視覺化。\",\"slug\":\"about\",\"locale\":\"zh\"}},{\"slug\":\"works\",\"isPage\":true,\"hideRSS\":false,\"en\":{\"title\":\"My Projects\",\"isPage\":\"true\",\"description\":\"A few selected works and write-ups of projects worth sharing.\",\"slug\":\"works\",\"locale\":\"en\"},\"zh\":{\"title\":\"我的作品集\",\"isPage\":\"true\",\"description\":\"A few selected works and write-ups of projects worth sharing.\",\"slug\":\"works\",\"locale\":\"zh\"}}]"}</script>
			<script type="application/json" data-sveltekit-fetched data-url="/content/intro-to-hci-lecture-6-testing.json">{"status":200,"statusText":"","headers":{},"body":"{\"en\":{\"title\":\"Intro to HCI Lecture 6 – Testing\",\"publishDate\":\"2015/04/04\",\"author\":\"Daniel Kao\",\"description\":\"These are my notes in essay form from the Intro to Human Computer Interaction course taught by Scott Klemmer at the University of California, San Diego.\",\"featureImage\":\"/images/2020/10/2015-01-31-03.32.46-1.jpg\",\"content\":[{\"type\":\"text\",\"value\":\"\u003Cp>These are my notes in essay form from the Intro to Human Computer Interaction course taught by \u003Ca href=\\\"https://d.ucsd.edu/srk/\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">Scott Klemmer\u003C/a> at the University of California, San Diego. All credit for content goes to Scott, any errors are my own.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>The next step in the process is to conduct tests on the actual usage of the application. Just like what we saw in the needfinding and interviewing process, simply asking people what they want will lead you to miss a lot of things. Questions like “do you like the interface?” tell you very little about how to improve your product. Even questions that appear to drive more specific results, such as rating the interface on a scale of one to ten, may appear to be more scientific, but is essentially the same question.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>One reason that these questions are relatively unhelpful is because they are not very specific. There is often nothing to compare an interface to when asking that question, leaving the experimenter with a subjective answer.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>There is also what is known as the “please the experimenter bias”, which is the effect of the tester doing their best to please the experimenter, which can be exacerbated when the experimenter is from a different social background, such as when white researchers conducted interviews in foreign countries like India.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Ch2 id=\\\"characteristics-of-good-research-questions\\\">Characteristics of Good Research Questions\u003C/h2>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>As with user interviews and getting feedback, getting beyond “do you like my interface” means asking questions to that are specific and provide specific insights. Here are a couple characteristics of good questions.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cul>\\n\u003Cli>\u003Cstrong>Baserates\u003C/strong> – Measuring a baserate generally involves measuring how often something happens, such as how many people click the first link on a search results page.\u003C/li>\\n\u003Cli>\u003Cstrong>Correlations\u003C/strong> – Measuring correlations means discovery connections between two separate events, such as measuring the correlation between when a class is offered and how many students attend.\u003C/li>\\n\u003Cli>\u003Cstrong>Causes\u003C/strong> – However, with any correlation, be aware that there can be multiple explanations. For whether or not students attend class, the correlation can be a function of when students want to go to class or when good professors want to teach.\u003C/li>\\n\u003C/ul>\\n\"},{\"type\":\"text\",\"value\":\"\u003Ch2 id=\\\"research-terms-to-know\\\">Research Terms to Know\u003C/h2>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>To fully understand the way that research is conducted, there are a couple of research terms to be aware of.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cul>\\n\u003Cli>\u003Cstrong>Independent variables\u003C/strong> are variables that are independent of what the user does, because they are manipulated by the experimenter or application. In the example of the experiment of classes, the independent variable is the time the class is, because that is what the experimenter has control over.\u003C/li>\\n\u003Cli>\u003Cstrong>Dependent variables\u003C/strong>, on the other hand, are variables that the user has control of and that the experimenter is trying to measure. Common dependent variables include how long it takes for a user to complete a task, how many errors people make, how much a person remembers after an action, or emotional response.\u003C/li>\\n\u003Cli>\u003Cstrong>Internal validity\u003C/strong> determines how reliable an experiment is. If you were to run the experiment again, would you get similar results? Or, if you were to run the experiment with a greater number of people, would the results be similar?\u003C/li>\\n\u003Cli>\u003Cstrong>External validity\u003C/strong> is the generalizability of the results. Because internal validity may not apply in a real world situation, external validity determines how easily the results can be applied to other situations.\u003C/li>\\n\u003C/ul>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>One of the common questions that designers want to ask is “is my cool new approach better than the industry standard?” After all, that is the reason why we are building something new. However, the problem with this question is that it is not a fair comparison. The industry standard is likely to have years of development and improvement as well as users that have adapted to the standard, while your product is likely a brand new one without too much development and familiarity. Consequently, asking if your approach is better than the industry standard can generate a response that is attributed to fidelity, approach, or a combination of both.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>For example, one research study that came out when the iPhone was launched seeked to see if the touchscreen keyboard on the iPhone is better than physical qwerty keyboards or numeric keyboards.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cblockquote>\\n\u003Cp>“Research firm User Centric has released a study that tries to gauge how effective the iPhone’s unusual on-screen keyboard is. The goal is certainly a noble one, but I can’t say that the survey’s approach results in data that makes much sense.\u003C/p>\\n\u003C/blockquote>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cblockquote>\\n\u003Cp>User Centric brought in twenty owners of other phones — half who had ones with QWERTY keyboards, and half who had ordinary numeric phone keypads. None were familiar with the iPhone. The research involved having the test subjects enter six sample text messages with the phones they already had, and six with an iPhone.\u003C/p>\\n\u003C/blockquote>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cblockquote>\\n\u003Cp>Logical end result: These iPhone newbies took twice as long to enter text with an iPhone as they did with their own phones, and made lots more typos.”\u003C/p>\\n\u003C/blockquote>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Let’s break down this study to see why it was not a very good study. In this study, the independent variable, or the manipulation, was the input style. The measure, or metric for comparison, was words per minute.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>In this study, all of the participants were new to iPhone keyboards, making them novices at the iPhone’s new style of keyboard, while being experts at the other type of keyboard they were already used to. While there is nothing wrong with testing beginners to see how fast people can learn a new keyboard, comparing their performance to a keyboard that they are used to is an intrinsically unfair comparison.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>When the study was performed again on participants a month later, they found that iPhone users were about as fast as QWERTY users. This comparison is a more equivalent comparison than the first one.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>However, when looking at the error rate of each type of input device, we find that iPhone users make more errors than the QWERTY or numeric keyboard users. Based on the nature of this study, this result can potentially be traced back to the fact that each keyboard type was used by a different group of users. Thus, there could have been significant differences in who each group of people were, making the variance in the user population still a problematic function if we want to draw conclusions about the overall performance of an input style.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Ch2 id=\\\"strategies-for-fair-comparisons\\\">Strategies for Fair Comparisons\u003C/h2>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Now that you’ve seen different ways that comparisons can be unfair, it is easier to recognize that an unfair comparison can lead to skewed results and inaccurate conclusions. Therefore, your job as a researcher is to make comparisons as fair as possible.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>The first strategy for a fairer comparison is to insert your new approach into the actual production setting. By swapping out only the part of the approach that you are trying to change, you can easily see the difference that your approach makes. This may seem challenging, but you can use proxy servers or client side scripting to easily make this work.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>The second strategy for a fairer comparison is to recreate the production approach in your new setting. Similar to the first strategy, this is another way to compare only the parts that you want to change into the production approach.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Another strategy that is used commonly in research is to scale things down so that you’re just looking at a piece of a larger system. Instead of building the entirety of the program, scaling things down to only the action that your change affects allows you to easily compare the differences.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Finally, when expertise is relevant, train people up. As we see in the study with the iPhone keyboards, people who were using the iPhone keyboards for the first time unsurprisingly performed worse than people who were using keyboards that they were familiar with.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>The answer to whether interface x is better than interface y is most often “it depends”. Without defining what “better” means, it is impossible to quantitatively compare two interfaces. On the contrary, by properly understanding these characteristics of experiments, it allows you to make more accurate conclusions on the data that you collect.\u003C/p>\\n\"},{\"type\":\"image\",\"value\":{\"src\":\"/images/2021/01/correlation.png\",\"alt\":\"Correlation Comic\",\"width\":\"400\",\"crops\":[]}},{\"type\":\"text\",\"value\":\"\u003Ch2 id=\\\"in-person-experiments\\\">In Person Experiments\u003C/h2>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>While the web has made running experiments online extremely easy, there are also many benefits that come with watching people in person. By watching people live, you can talk to them about what they’re doing, hear their feedback, see points of confusion, and in general have a higher bandwidth of engagement.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>When planning an in person experiment, the first step is to make clear goals and lay out the scope of the experiment, which will be guided by the purpose of the experiment, or what you hope to learn.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>For example, if you’re building a room reservation system for a computer science department, one strategy would be to put all the information on one page, while another strategy may be putting information on multiple pages. You might be interested in seeing if splitting things up increases the likelihood of booking a room or not.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>To conduct this experiment, we must consider a couple of factors. First, identify an appropriate setting for testing. If your software is most likely to be used in a quiet room, find a quiet room to do your testing. If your software is most likely to be used at a train station, conduct your experiment at a train station.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Next, figure out who you’re going to recruit, how, and how many people. Come up with realistic scenarios, ideally ones that participants can care about, to give people when they come in.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>When it comes time for people to come in to test your product, have a list of questions and things that you want to pay attention to. In the case of our example, questions may include “Will people book the right sized room? For the right length of time?” or “How long did it take for the participant to book the room?” or “What other actions do people do while booking a room?”\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>If you can, it is helpful to have at least two experimenters present for the test. One person will be in charge of facilitating while the other takes notes.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>During in person experiments, it is important to create concrete tasks for people to do, so that you have concrete data to compare, and everyone gets the same experience. In our example, a concrete task prompt might look something like:\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cblockquote>\\n\u003Cp>“Book a room sometime next week for a research group meeting. Andrew will be out of town, so we won’t hear his weekly update. The rest of us should be present and give our updates. Besides the usual group members, we’ll have two visitors from France who will present their research — maybe they’ll take ten minutes each. When you’re done booking the room, tell Arvind so he can prepare the next task for you.”\u003C/p>\\n\u003C/blockquote>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>As a side note, something to consider when performing in person experiments are ethical considerations. Some experiments have left participants in tears when they were unable to perform well. For starters, make consent voluntary, and make sure consent is informed, and allow participants to stop the study whenever they want. Perhaps the biggest clarification is to inform the user that you are testing the site, not the users. This helps users to understand that mistakes are the fault of the system instead of their own.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Another tip to create better experiments is to pilot your experimenting, letting one or two people go through the experiment before bringing in larger amounts of participants. This will help you work out any kinks in the flow of the experiment, the order of the experiment, and any other cases that may arise.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>To best capture results from your experiment, the most simple and direct way is to take notes of things that went well, questions that arose, or problems to fix, one option is to record video. Be sure to collect both qualitative process data such as observations of what users are thinking and doing, as well as bottom-line quantitative data such as how many people succeed, how long it takes, etc.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Going beyond simple note taking, you may choose to record video. Recording video allows you to document and easily share exactly how and what the user felt as they go through the process.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Another consideration to capturing results is to determine whether you want participants to think aloud while they are going through the process. This allows you to record what the users are thinking, not just what they are doing. Since most people may not be used to sharing their thoughts out loud, you may need to prompt users along the way. Be aware that thinking out loud may also cause users behavior’s to change, and may not be an exact representation of what the user is thinking.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>At the end of the experiment, it is helpful to debrief the participant so that they know what your goals were and so that you can learn more holistically what they are thinking after the experiment.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Ch2 id=\\\"online-experiments\\\">Online Experiments\u003C/h2>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>The web has enabled tremendous power to conduct experiments online. This is accomplished by rolling out different versions of the user interface, getting feedback, and iterating quickly. You may have heard of this under many different names, likely A/B testing, split testing, randomized experiments, etc. The basic premise for this kind of testing is to randomly split traffic on your website to two or more versions of your interface, and then collect data on how the two versions perform.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>As an example, the following are pages for the national alert registry, with three versions laid out below. The third interface, a two column layout, was designed to see whether it would generate more sign ups because the content is available above the fold.\u003C/p>\\n\"},{\"type\":\"image\",\"value\":{\"src\":\"/images/2021/01/Screenshot-2015-04-03-20.14.10.png\",\"className\":\"full\",\"alt\":\"Screenshot of national alert registry\",\"crops\":[{\"image\":\"/images/2021/01/Screenshot-2015-04-03-20.14.10.png\",\"width\":1920,\"format\":\"png\"},{\"image\":\"/images/2021/01/Screenshot-2015-04-03-20.14.10.png\",\"width\":1280,\"format\":\"png\"},{\"image\":\"/images/2021/01/Screenshot-2015-04-03-20.14.10.png\",\"width\":768,\"format\":\"png\"},{\"image\":\"/images/2021/01/Screenshot-2015-04-03-20.14.10.png\",\"width\":640,\"format\":\"png\"}]}},{\"type\":\"text\",\"value\":\"\u003Cp>Before scrolling down to see the results, think about which version of these three user interfaces you think was the most effective.\u003C/p>\\n\"},{\"type\":\"image\",\"value\":{\"src\":\"/images/2021/01/Screenshot-2015-04-03-20.13.37.png\",\"alt\":\"Screenshot of table\",\"crops\":[{\"image\":\"/images/2021/01/Screenshot-2015-04-03-20.13.37.png\",\"width\":768,\"format\":\"png\"},{\"image\":\"/images/2021/01/Screenshot-2015-04-03-20.13.37.png\",\"width\":640,\"format\":\"png\"}]}},{\"type\":\"text\",\"value\":\"\u003Cp>What you can see from the data is that the third version actually performed worse than the first two. Although it was intended to be more effective, the number of sales actually dropped tremendously. Even the very best designers often make a revision that performs worse than the original. The web allows us to easily try out these designs and get real feedback on how the designs perform.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>So why did the third version do so much worse? One theory is the word free at the top left of the screen, that can dissuade people from buying something. Another theory is that by creating two columns, there is less of a clear flow, creating a perception of multiple options instead of a clear funnel that you want people to go through. By contrast, the second version did not have the word free at the top, and had a stronger call to action.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Therefore, we can see from this example that small changes can have a big effect on the performance of a site. These changes can include things like the position and color of the primary call to action, position of testimonials on the page, whether linked elements are in text or images, the amount of white space on a page, the position and prominence of the main heading, the number of columns on the page, number of visual elements competing for attention, the age, sex, and appearance of someone in a photo, etc. See the \u003Ca href=\\\"/intro-to-hci-lecture-5-visual-design\\\">visual design\u003C/a> lecture for more details on how to create good visual design.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Another example is Obama’s presidential campaign in 2008. Dan Siroker of the Obama campaign, shares how he used simple A/B tests on the button as well as the media to increase the performance of the Obama’s campaign site.\u003C/p>\\n\"},{\"type\":\"embed\",\"value\":\"\u003Ciframe width=\\\"100%\\\" height=\\\"400\\\" src=\\\"https://www.youtube.com/embed/71bH8z6iqSc\\\" title=\\\"YouTube video player\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen>\u003C/iframe>\"},{\"type\":\"text\",\"value\":\"\u003Cp>In the experiment with the text on the buttons, the only thing that changed between versions is the text on the button. The four choices were “Sign Up”, “Learn More”, “Join Us Now”, and “Sign Up Now”. Turns out, “Learn More” performed 18% better than the baseline.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>The second experiment, the media on the page, was split between three photos and three videos. The strongest media, contrary to what most people would have guessed, was the family photo.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>From these examples, there are a couple of general principles that contribute to a quality online experiments.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Firstly, when running online experiments, it is most helpful to run experiments at 50/50%. While some people recommend starting a small percentage of users with the new design, this is not as helpful for being able to quickly tell which interface performs better. By setting it at 50/50, the better interface is blatantly apparent. However, to prevent disastrous results, ramping up to 50% is the best course of action. Ramping up will help avoid software bugs or other unforeseen effects affecting users from performing the task.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>There are some key rules for random assignment. The first is that your assignment should be consistent. The same person should see the same interface every single time they use your interface. The second key rule is that your assignment should be truly and completely random. This means that you want to run different versions simultaneously, and that the probability someone has of using one version of the interface is the same as using the another version.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>The hardest part about online experimentation is picking the metric to measure a good interface by. One common mistake when picking metrics is picking a metric that is easy to measure instead of a metric that accurately reflects success. Thus, it is important to pick a metric that matters over picking a metric that is easy to measure.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Additionally, it is also important to run online experiments for long enough. Just like users testing the new iPhone keyboard, if you change a user interface, people may not be used to it, generating poorer initial results.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Finally, no matter how long or diligently you conduct your experiments, larger theories are still difficult to prove. Even with large sets of data, the variance and changes can be due to a larger, unseen reason. Thus, it is important to constantly ask different questions and run multiple types of experiments to identify trends on how interfaces perform rather than all encompassing claims one way or the other.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Ch2 id=\\\"interpreting-test-results\\\">Interpreting Test Results\u003C/h2>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>After you’ve done your tests, whether in person or online, it is important to properly analyze the data so that you reach accurate conclusions. A good starting point is asking yourself the questions “What does my data look like?”, “What are the overall numbers?” and “Are the differences real?”\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>For instance, say I have a coin and I want to test to see whether the coin is a fair coin or not. To test the coin for fairness, let’s say that I flip it 20 times and 13 of the flips came up heads. Despite the fact that the result of 13 heads is not equal to the expected number of heads, we need to determine if 3 additional heads out of 20 is a significant difference.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Since the results of flipping a coin lies on a gaussian distribution, that is that the further away the outcome is from the mean, the less likely it is to occur, we know that flipping 11 heads on a fair coin out of 20 flips is more likely than flipping 19 heads out of 20 flips.\u003C/p>\\n\"},{\"type\":\"image\",\"value\":{\"src\":\"/images/2021/01/fig11-10.jpg\",\"alt\":\"normal distribution\",\"width\":\"300\",\"crops\":[]}},{\"type\":\"text\",\"value\":\"\u003Cp>To determine the significance of the difference, we are going to use the \u003Ca href=\\\"https://en.wikipedia.org/wiki/Pearson's_chi-squared_test\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">Pearson’s Chi-Squared Test\u003C/a>. This test is a way to compare the expected rates to the experimental rates to determine a percentage of confidence toward a conclusion.\u003C/p>\\n\"},{\"type\":\"image\",\"value\":{\"src\":\"/images/2021/01/latex.png\",\"alt\":\"confidence equation x^{2} = \\\\frac{(observed - expected)^{2}}{expected}\",\"width\":\"300\",\"crops\":[]}},{\"type\":\"text\",\"value\":\"\u003Cp>To accurately determine whether we can reject the null hypothesis, or the default hypothesis that the coin is a fair coin, we need a threshold p-value, most often 0.05, to compare with the result of the chi-squared test.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Therefore, if 20 flips of a coin resulted in 13 heads and we have a p-value less than 0.05, we have:\u003C/p>\\n\"},{\"type\":\"image\",\"value\":{\"src\":\"/images/2021/01/latex--1-.png\",\"alt\":\"\\\\frac{(13 - 10)^{2}}{10} + \\\\frac{(7 - 10)^{2}}{10} = 1.8\",\"width\":\"300\",\"crops\":[]}},{\"type\":\"text\",\"value\":\"\u003Cp>(heads + tails = chi-squared value)\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Based on our calculation and the \u003Ca href=\\\"http://www.socscistatistics.com/pvalues/chidistribution.aspx\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">p-value distribution table\u003C/a>, we can see that our results are not quite at the p &lt; 0.05 threshold, meaning that we cannot sufficiently reject our null hypothesis and say that the coin is biased.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>However, if we flipped the coin 60 times and resulted in 39 heads, the same ratio as the previous experiment, running those numbers through the chi-squared distribution allows us to reject the null hypothesis with 98% certainty. This is because more trials brings our chi-squared value up.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Chi-Squared testing is valuable because it doesn’t just apply to coins, but also applies to click through rates on websites. Let’s say that a website has a button labeled “sign up”, and 10% of visitors click the button. To try to improve traffic, designers change the button to “learn more”, and start gathering data. Over a week, there were 1000 visitors to the site, 118 of which clicked the “learn more” button. Running this data through the formula gives us the follow chi-squared value:\u003C/p>\\n\"},{\"type\":\"image\",\"value\":{\"src\":\"/images/2021/01/latex--2-.png\",\"alt\":\"\\\\frac{(118 - 100)^{2}}{100} + \\\\frac{(882 - 900)^{2}}{900} = 4.01\",\"width\":\"300\",\"crops\":[]}},{\"type\":\"text\",\"value\":\"\u003Cp>In looking up this data in \u003Ca href=\\\"http://www.socscistatistics.com/pvalues/chidistribution.aspx\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">our table\u003C/a>, we find that 4.01 generates a p-value that is just smaller than 0.05, allowing us to formally say that the change was an improvement.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Thus, statistical testing gives you two very powerful tools. First, it allows you to formalize “we’re pretty sure”, using statistics to validate your claim. Secondly, it helps you generalize (or not) from small sample sizes. There are also other tests that achieve a similar purpose for continuous data such as \u003Ca href=\\\"https://en.wikipedia.org/wiki/Student's_t-test\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">T-tests\u003C/a> and \u003Ca href=\\\"https://en.wikipedia.org/wiki/Analysis_of_variance\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">ANOVA\u003C/a>.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cp>Also, note that data often isn’t “normal”. Data doesn’t always lie on a gaussian distribution, as it can be bimodal or weighted one way, making statistical analysis potential complex.\u003C/p>\\n\"},{\"type\":\"text\",\"value\":\"\u003Ch2 id=\\\"further-reading-and-resources\\\">Further Reading and Resources\u003C/h2>\\n\"},{\"type\":\"text\",\"value\":\"\u003Cul>\\n\u003Cli>\u003Ca href=\\\"https://faculty.washington.edu/wobbrock/pubs/hcic-11.pdf\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">Practical Statistics for HCI – Jacob Wobbrock\u003C/a>\u003C/li>\\n\u003Cli>\u003Ca href=\\\"https://www.amazon.com/gp/product/0495115770?linkId=I66BIBQDKUFJCBKA\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">Doing Psychology Experiments – David W Martin\u003C/a>\u003C/li>\\n\u003Cli>\u003Ca href=\\\"https://www.amazon.com/gp/product/0805805281?linkId=YEHNHW7DK43YYWRL\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">Statistics as Principled Argument – Robert P. Abelson\u003C/a>\u003C/li>\\n\u003Cli>\u003Ca href=\\\"https://www.amazon.com/gp/product/0335216803?linkId=YGYS6V3HD5CQXE7L\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">Learning to use Statistical Tests in Psychology – Judith Greene, Manuela D’Oliveira\u003C/a>\u003C/li>\\n\u003C/ul>\\n\"}],\"featureImageCrops\":[{\"image\":\"/images/2020/10/2015-01-31-03.32.46-1.jpg\",\"width\":1920,\"format\":\"jpg\"},{\"image\":\"/images/2020/10/2015-01-31-03.32.46-1.jpg\",\"width\":1280,\"format\":\"jpg\"},{\"image\":\"/images/2020/10/2015-01-31-03.32.46-1.jpg\",\"width\":768,\"format\":\"jpg\"},{\"image\":\"/images/2020/10/2015-01-31-03.32.46-1.jpg\",\"width\":640,\"format\":\"jpg\"}],\"slug\":\"intro-to-hci-lecture-6-testing\",\"locale\":\"en\"}}"}</script>
			<script>
				{
					__sveltekit_1e4tdh9 = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("../_app/immutable/entry/start.ChEkEh4p.js"),
						import("../_app/immutable/entry/app.7Jp1DJ0Y.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 4],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		
  </body>
</html>
